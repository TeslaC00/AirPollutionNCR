{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "RAW_DATA_DIR_PATH = \"G:\\\\My Drive\\\\AirPollutionML\\\\Raw_Data\"\n",
    "# RAW_DATA_FILES_PATH = 'G:\\\\My Drive\\\\AirPollutionML\\\\Raw_Data_Files'\n",
    "RAW_DATA_FILES_PATH = \"data\"\n",
    "RAW_FILE_PATTERN = r\"(?:Copy of )?Raw_data_1Hr_(\\d{4})_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regex import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly Region Files Aggregation\n",
    "Files of each year by each region with mean of station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_files = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(RAW_DATA_FILES_PATH):\n",
    "    match = re.match(STATION_FILE_PATTERN, filename)\n",
    "    if match:\n",
    "        year, region, station = match.groups()\n",
    "        if year not in station_files:\n",
    "            station_files[year] = {}\n",
    "        if region not in station_files[year]:\n",
    "            station_files[year][region] = []\n",
    "        station_files[year][region].append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, regions in station_files.items():\n",
    "    for region, files in regions.items():\n",
    "        new_filename = f\"{year}_{region}.csv\"\n",
    "        aggregated_df = pd.concat(\n",
    "            [pd.read_csv(os.path.join(RAW_DATA_FILES_PATH, file)) for file in files]\n",
    "        )\n",
    "        aggregated_df = aggregated_df.groupby(\"Timestamp\").mean()\n",
    "        aggregated_df = aggregated_df.reset_index().rename(\n",
    "            columns={\"index\": \"Timestamp\"}\n",
    "        )\n",
    "        aggregated_df.to_csv(\n",
    "            os.path.join(RAW_DATA_FILES_PATH, new_filename), index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year Files Aggregation\n",
    "Files of each year with mean of all regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_files = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(RAW_DATA_FILES_PATH):\n",
    "    match = re.match(YEAR_REGION_FILE_PATTERN, filename)\n",
    "    if match:\n",
    "        year, region = match.groups()\n",
    "        if year not in year_files:\n",
    "            year_files[year] = []\n",
    "        year_files[year].append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2023': ['2023_Gurugram.csv',\n",
       "  '2023_Faridabad.csv',\n",
       "  '2023_Delhi.csv',\n",
       "  '2023_Sonipat.csv',\n",
       "  '2023_Rohtak.csv',\n",
       "  '2023_Jind.csv',\n",
       "  '2023_Meerut.csv',\n",
       "  '2023_Muzaffarnagar.csv',\n",
       "  '2023_Ghaziabad.csv',\n",
       "  '2023_Baghpat.csv',\n",
       "  '2023_Palwal.csv',\n",
       "  '2023_Karnal.csv',\n",
       "  '2023_Noida.csv'],\n",
       " '2022': ['2022_Gurugram.csv',\n",
       "  '2022_Faridabad.csv',\n",
       "  '2022_Delhi.csv',\n",
       "  '2022_Sonipat.csv',\n",
       "  '2022_Rohtak.csv',\n",
       "  '2022_Jind.csv',\n",
       "  '2022_Meerut.csv',\n",
       "  '2022_Muzaffarnagar.csv',\n",
       "  '2022_Ghaziabad.csv',\n",
       "  '2022_Baghpat.csv',\n",
       "  '2022_Palwal.csv',\n",
       "  '2022_Karnal.csv',\n",
       "  '2022_Noida.csv'],\n",
       " '2021': ['2021_Gurugram.csv',\n",
       "  '2021_Faridabad.csv',\n",
       "  '2021_Delhi.csv',\n",
       "  '2021_Sonipat.csv',\n",
       "  '2021_Rohtak.csv',\n",
       "  '2021_Jind.csv',\n",
       "  '2021_Meerut.csv',\n",
       "  '2021_Muzaffarnagar.csv',\n",
       "  '2021_Ghaziabad.csv',\n",
       "  '2021_Baghpat.csv',\n",
       "  '2021_Palwal.csv',\n",
       "  '2021_Karnal.csv',\n",
       "  '2021_Noida.csv'],\n",
       " '2020': ['2020_Gurugram.csv',\n",
       "  '2020_Faridabad.csv',\n",
       "  '2020_Delhi.csv',\n",
       "  '2020_Sonipat.csv',\n",
       "  '2020_Rohtak.csv',\n",
       "  '2020_Jind.csv',\n",
       "  '2020_Meerut.csv',\n",
       "  '2020_Muzaffarnagar.csv',\n",
       "  '2020_Ghaziabad.csv',\n",
       "  '2020_Baghpat.csv',\n",
       "  '2020_Palwal.csv',\n",
       "  '2020_Karnal.csv',\n",
       "  '2020_Noida.csv'],\n",
       " '2019': ['2019_Gurugram.csv',\n",
       "  '2019_Faridabad.csv',\n",
       "  '2019_Delhi.csv',\n",
       "  '2019_Sonipat.csv',\n",
       "  '2019_Rohtak.csv',\n",
       "  '2019_Jind.csv',\n",
       "  '2019_Meerut.csv',\n",
       "  '2019_Muzaffarnagar.csv',\n",
       "  '2019_Ghaziabad.csv',\n",
       "  '2019_Baghpat.csv',\n",
       "  '2019_Palwal.csv',\n",
       "  '2019_Karnal.csv',\n",
       "  '2019_Noida.csv']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\My Drive\\\\AirPollutionML\\\\Raw_Data_Files'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAW_DATA_FILES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, files in year_files.items():\n",
    "    new_filename = f\"{year}.csv\"\n",
    "    aggregated_df = pd.concat(\n",
    "        [pd.read_csv(os.path.join(RAW_DATA_FILES_PATH, file)) for file in files]\n",
    "    )\n",
    "    aggregated_df = aggregated_df.groupby(\"Timestamp\").mean()\n",
    "    aggregated_df = aggregated_df.reset_index().rename(columns={\"index\": \"Timestamp\"})\n",
    "    aggregated_df.to_csv(os.path.join(RAW_DATA_FILES_PATH, new_filename), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year All Region Files Aggregation\n",
    "Files of each year and all regions as column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, files in year_files.items():\n",
    "    dfs = []\n",
    "    new_filename = f\"{year}_all_regions.csv\"\n",
    "    for file in files:\n",
    "        match = re.match(YEAR_REGION_FILE_PATTERN, file)\n",
    "        if match:\n",
    "            df = pd.read_csv(os.path.join(RAW_DATA_FILES_PATH, file))\n",
    "            _, region = match.groups()\n",
    "            df[\"Region\"] = region\n",
    "            dfs.append(df)\n",
    "        else:\n",
    "            print(\n",
    "                \"Error in matching this file: {} with this pattern {}\",\n",
    "                file,\n",
    "                YEAR_REGION_FILE_PATTERN,\n",
    "            )\n",
    "    aggregated_df = pd.concat(dfs)\n",
    "    aggregated_df.to_csv(os.path.join(RAW_DATA_FILES_PATH, new_filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(os.path.join(RAW_DATA_FILES_PATH,\"2019_all_regions.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region All Year Files Aggregation\n",
    "Files of each region and all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_files = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(RAW_DATA_FILES_PATH):\n",
    "    match = re.match(YEAR_REGION_FILE_PATTERN, filename)\n",
    "    if match:\n",
    "        year, region = match.groups()\n",
    "        if region not in regions_files:\n",
    "            regions_files[region] = []\n",
    "        regions_files[region].append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region, files in regions_files.items():\n",
    "    dfs = []\n",
    "    new_filename = f\"{region}_all_years.csv\"\n",
    "    aggregated_df = pd.concat(\n",
    "        [pd.read_csv(os.path.join(RAW_DATA_FILES_PATH, file)) for file in files]\n",
    "    )\n",
    "    aggregated_df.to_csv(os.path.join(RAW_DATA_FILES_PATH, new_filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(os.path.join(RAW_DATA_FILES_PATH,\"Delhi_all_years.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
